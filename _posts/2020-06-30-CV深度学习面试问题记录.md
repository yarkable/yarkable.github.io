---
layout: post
title: CV深度学习面试问题记录
subtitle: 
date: 2020-06-30
author: kevin
header-img: img/green-bg.jpg
catalog: true
tags:
    - deep learning
    - job
---



## preface



这是在牛客网上根据大家的面经收集过来的题目，并以自己的理解来作出回答，也查阅了很多博客和资料。水平有限，不一定是正确的，欢迎指正，铁子们要找工作的时候可以看看



## 图像细粒度分类是什么



不同于普通的分类任务，图像细粒度分类是指的对同一个大类别进行更加细致的分类，例如哈士奇与柯基。



## RPN是怎么做的



有空补上

![img](https://pic2.zhimg.com/80/v2-38ecc58507df271897fdae605868d6e1_720w.png)



## 过拟合欠拟合是啥，怎么解决？



### 过拟合



过拟合是指训练误差和测试误差之间的差距太大。模型在训练集上表现很好，但在测试集上却表现很差，也就是泛化能力差。引起的原因有 ①训练数据集样本单一，样本不足 ②训练数据中噪声干扰过大 ③模型过于复杂。



防止过拟合的几种方法：

1. 增加更多的训练集或者数据增强(根本上解决问题)
2. 采用更简单点的模型(奥卡姆剃刀准则，简单的就是正确的)
3. 正则化(L1，L2，Dropout)
4. 可以减少不必要的特征或使用较少的特征组合



### 欠拟合



欠拟合是指模型不能在训练集上获得足够低的误差。换句换说，就是模型复杂度低，模型在训练集上就表现很差，没法学习到数据背后的规律。



欠拟合基本上都会发生在训练刚开始的时候，经过不断训练之后欠拟合应该不怎么考虑了。欠拟合可以通过使用非线性模型来改善，也就是使用一个更强的模型，并且可以增加训练特征



## ResNet为啥能够解决梯度消失？怎么做的，能推导吗？



## 深度学习里面PCA的作用



PCA 用于降低维度，消除数据的冗余，减少一些不必要的特征。常用步骤如下（设有 m 条 n 维数据）：

1. 将原始数据按照列组成 m 行 n 列矩阵 X
2. 将 X 的每一列(代表一个属性字段)进行零均值化(去中心化)减去这一列的均值得到特征中心化后的矩阵 $B = X_i - \mu_i$
3. 求出协方差矩阵 $C = (X_i - \mu_i)*(X_i - \mu_i)^T$
4. 求出矩阵 C 的特征值($det|A-\lambda I|=0$)和特征向量，按照特征值的大小将对应的特征向量从左往右排列，取前 k 个最大的特征值对应的特征向量组成矩阵 P
5. Y = PX 即为降维到 k 维后的数据，即从 n 维降到了 k 维，保留了原本的大部分信息



> reference:
>
> 王兴政老师的PPT
>
> https://blog.csdn.net/Virtual_Func/article/details/51273985



## YOLOv3的框是怎么聚出来的？



YOLOv3 的框是作者通过对自己的训练集的 bbox 聚类聚出来的，本质上就是一个 kmeans 算法，原理如下：

1. 随机选取 k 个类别的中心点，代表各个聚类
2. 计算所有样本到这几个中心点的距离，将每个样本归为距离最小的类别
3. 更新每个类别的中心点(计算均值)
4. 重新进行步骤 2 ，直到类的中心点不再改变

![kmeans.png](https://i.loli.net/2020/07/01/dBGSP9543eQJTka.png)



YOLOv3 的 kmeans 想法不能直接用 w 和 h 来作为聚类计算距离的标准，因为 bbox 有大有小，不应该让 bbox 的尺寸影响聚类结果，因此，YOLOv3 的聚类使用了 IOU 思想来作为距离的度量
$$
d(bbox, centroid) = 1 - IOU(bbox, centroid)
$$
也就是说 IOU 越大的话，表示 bbox 和 box_cluster 就越类似，于是将 bbox 划归为 box_cluster



在 AlexeyAB 大佬改进的 darknet 中实现了这个过程，具体就是加载所有训练集中的 bbox 的 w 和 h，随机选择 k 个作为 centroids，然后用所有样本去和这 k 个 centroids 做 IOU 得出距离，并将样本归为距离最小的 cluster，然后更新 centroids 重复上面的步骤，直到 centroids 不再更新



> reference:
>
> https://github.com/AlexeyAB/darknet/blob/master/scripts/gen_anchors.py
>
> https://www.cnblogs.com/sdu20112013/p/10937717.html



## YOLO和SSD的本质区别？



## R-CNN系列和SSD本质有啥不一样吗？



## SSD的致命缺点？如何改进



需要人工设置 prior box 的 min_size，max_size 和 aspect_ratio 值。网络中prior box的基础大小和形状不能直接通过学习获得，而是需要手工设置。而网络中每一层 feature 使用的 prior box 大小和形状恰好都不一样，导致调试过程非常依赖经验。
虽然采用了 pyramdial feature hierarchy 的思路，但是对小目标的 recall 依然一般，并没有达到碾压 Faster RCNN 的级别。作者认为，这是由于 SSD 使用 conv4_3 低级 feature 去检测小目标，而低级特征卷积层数少，存在特征提取不充分的问题。Kevin 补充：没错，SSD 的 anchor 设置导致 anchor 和小目标的 IoU 依然很小，大概只有 0.2 左右所以会漏掉很多小物体。
————————————————
版权声明：本文为CSDN博主「一个新新的小白」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/qq_31511955/article/details/80597211



## SSD 的先验框和 Gt 框是怎么匹配的



在训练时 首先需要确定训练图片中的 ground truth 是由哪一个先验框来匹配，与之匹配的先验框所对应的边 界框将负责预测它。SSD 的先验框和 ground truth 匹配原则主要有 2 点。第一点是对于图片中的每个 ground truth，找到和它 IOU 最大的先验框，该先验框与其匹配，这样可以保证每个 ground truth 一 定与某个 prior 匹配。第二点是对于剩余的未匹配的先验框，若某个 ground truth 和它的 IOU 大于某 个阈值 (一般设为 0.5)，那么这个先验框匹配这个 ground truth，剩下没有匹配上的先验框都是负样本（如果多个 ground truth 和某一个先验框的 IOU 均大于阈值，那么先验框只与 IOU 最大的那个进行匹配）。



## 怎么解决梯度消失和梯度弥散



## YOLOv1到YOLOv3的发展历程以及解决的问题



## FCOS 存在什么问题，如何解决



1. 分配正负样本时，只要 anchor 在 gt_box 里面就算是正样本，这样会将很多背景区域也当成正样本，解决方式就是用 center_sampling，只在 gt_box 周围一个范围内的样本才是正样本。
2. 原始论文中，为了解决网络在不同 FPN 层之间回归的距离差异太大，假设 FPN 某个 stride feature map 上某个点的回归预测值为 x，则对应的值为 $\exp^{S_i*x}$​，使输出非负数，其中 $S_i$​​ 为当前尺度的一个可学习参数。这样做的话，用到指数函数，并且效果也不一定好，改进的点就是先将预测的距离通过一个 ReLU 使结果非负，然后在做标签的时候就将距离除以 stride，这样子的话每一层要回归的距离范围就差不多了，推理的时候只需要乘上 stride 就能还原真实 box 了。
3. centerness 没什么作用，直接预测 IoU 代替。
4. bbox loss weight 在原文中所有正样本的权重都是一样的，改进的话可以将该点的 centerness 值作为权重，越靠近中心点的 box 给越大的权重。



> [FCOS 的改进 trick - 极市社区 (cvmart.net)](https://bbs.cvmart.net/articles/3449)

## 为什么分类要用交叉熵损失函数



## 说一下RCNN发展史



RCNN -> Fast RCNN -> Faster RCNN

#TODO



### Faster RCNN

> 以经典的Faster R-CNN为例。整个网络可以分为两个阶段，training阶段和inference阶段
>
> training 阶段，RPN 网络提出了 2000 左右的 proposals，这些 proposals 被送入到 Fast R-CNN 结构中，在 Fast R-CNN 结构中，首先计算每个 proposal 和 gt 之间的 iou，通过人为的设定一个 IoU 阈值（通常为 0.5），把这些 Proposals 分为正样本（前景）和负样本（背景），并对这些正负样本采样，使得他们之间的比例尽量满足（1:3，二者总数量通常为 128），之后这些 proposals（128 个）被送入到 Roi Pooling，最后进行类别分类和 box 回归。inference 阶段，RPN 网络提出了 300 左右的 proposals，这些 proposals 被送入到 Fast R-CNN 结构中，和 training 阶段不同的是，inference 阶段没有办法对这些 proposals 采样（inference 阶段肯定不知道 gt 的，也就没法计算 iou），所以他们直接进入 Roi Pooling，之后进行类别分类和 box 回归





## NMS 的缺点以及改进工作



## Canny 算子的原理



## 边框回归怎么做的，为什么需要 Encode



Most recently object detection programs have the concept of anchor  boxes, also called prior boxes, which are pre-defined fix-sized bounding boxes on image input or feature map. The bounding box regressor,  instead of predicting the bounding box location on the image, predicts  the offset of the ground-truth/predicted bounding box to the anchor box. For example, if the anchor box representation is [0.2, 0.5, 0.1, 0.2],  and the representation of the ground-truth box corresponding to the  anchor box is [0.25, 0.55, 0.08, 0.25], the prediction target, which is  the offset, should be [0.05, 0.05, -0.02, 0.05]. The object detection  bounding box regressor is trying to learn how to predict this offset. If you have the prediction and the corresponding anchor box  representation, you could easily calculate back to predicted bounding  box representation. This step is also called decoding.



https://leimao.github.io/blog/Bounding-Box-Encoding-Decoding/

https://blog.csdn.net/zijin0802034/article/details/77685438

https://www.zhihu.com/question/370354869

有空写一下！！！！



gt 在做 loss 之前就已经和 Anchor 经过了 encode ，也就是从 Anchor 到 gt 所要经过的线性变换 `(Gx, Gy, Gw, Gh)`，回归网络的输出就是这四个变换，也就是说网络在学习这种映射关系。所以网络的输出并不是真正的坐标，得到坐标只需要根据 encode 得到 decode 函数做一下解码就行了。



而且也不是所有的 Anchor 都参与了回归损失（拿 SSD 中的 8732 个 Anchor 和 smooth L1 来说），最终可以计算得到一个 pos_inx 代表正样本的索引，根据这个索引在 gt 和 Anchor 中找到对应的元素拿来做回归损失，因为负样本不用回归。



## 常见的Loss，回归的，分类的，Focal Loss



### Sigmoid


$$
\sigma=\frac{1}{1+e^-x}
$$


值域为（0，1）



### Focal Loss

变形的交叉熵
$$
CE = -\alpha(1-y_{pred})^\gamma log(y_{pred})
$$


```python
def FocalLoss(self, logit, target, gamma=2, alpha=0.5):
    n, c, h, w = logit.size()
    criterion = nn.CrossEntropyLoss(weight=self.weight, ignore_index=self.ignore_index,
                                    size_average=self.size_average)
    if self.cuda:
        criterion = criterion.cuda()

        logpt = -criterion(logit, target.long())
        pt = torch.exp(logpt)
        if alpha is not None:
            logpt *= alpha
            loss = -((1 - pt) ** gamma) * logpt

            if self.batch_average:
                loss /= n

                return loss
```

>  reference: https://www.cnblogs.com/king-lps/p/9497836.html
>
>  [FocalLoss 对样本不平衡的权重调节和减低损失值 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/82148525) 这个和 CE BCE 进行对比，比较清晰，但是这两个都是基于 softmax 的 FocalLoss，mmdet 中默认是基于 sigmoid 的 BCE loss 作为原型进行 FocalLoss 的。



### softmax



常用于分类问题，在最后一层全连接之后用一个 softmax  将各神经元输出归一化到 0-1 之间，但总体相加的值还是等于 1


$$
\sigma(Z)_j = \frac{e^{z_j}}{\sum^K_{k=1}e{z_k}}
$$


```python
import numpy as np
z = np.array([1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0])
print(np.exp(z)/sum(np.exp(z)))
>>> [0.02364054 0.06426166 0.1746813 0.474833 0.02364054 0.06426166 0.1746813 ]
```



### BCELoss



二分类常用的 loss，Binary CrossEntropyLoss，注意，在使用这个之前要使用 sigmoid 函数将预测值放到（0，1）之间


$$
L = -\frac{1}{n}(ylog(y_{pred})+(1-y)log(1-y_{pred}))
$$


### BCEWithLogitsLoss



这是集成了 sigmoid 和 BCELoss 的损失函数，一步到位



### CrossEntropyLoss


$$
CE = -\frac{1}{n}ylog(y_{pred})
$$


其中，$y_{pred}$ 又是网络输出通过一个 softmax 得到的值，也就是说函数会自动给他加一个 softmax 函数，所以最终公式就是


$$
CE = -\frac{1}{n}ylog(\frac{e^{y_c}}{\sum_je^{y_j}})
$$


也就是 NLLLoss 和  LogSoftmax 两个函数的组合



```python
>>> i = torch.randn(3,3)
>>> i
tensor([[-1.8954, -0.3460, -0.6694],
        [ 0.5421, -1.1799,  0.8239],
        [-0.0611, -0.8800,  0.8310]])
>>> label = torch.tensor([[0,0,1],[0,1,0],[0,0,1]])
>>> label
tensor([[0, 0, 1],
        [0, 1, 0],
        [0, 0, 1]])
>>> sm = nn.Softmax(dim=1)
>>> sm(i)
tensor([[0.1097, 0.5165, 0.3738],
        [0.3993, 0.0714, 0.5293],
        [0.2577, 0.1136, 0.6287]])
>>> torch.log(sm(i))
tensor([[-2.2101, -0.6607, -0.9840],
        [-0.9180, -2.6400, -0.6362],
        [-1.3561, -2.1751, -0.4640]])
>>> ll = nn.NLLLoss()
>>> target = torch.tensor([2,1,2])
>>> ll(torch.log(sm(i)), target)
tensor(1.3627)

>>> i
tensor([[-1.8954, -0.3460, -0.6694],
        [ 0.5421, -1.1799,  0.8239],
        [-0.0611, -0.8800,  0.8310]])
>>> los = nn.CrossEntropyLoss()
>>> los(i, target)
tensor(1.3627)
```



## L1、L2范数是什么，区别呢？



范数是具有 “长度” 概念的函数。在向量空间内，为所有的向量的赋予非零的增长度或者大小。不同的范数，所求的向量的长度或者大小是不同的。举个例子，2 维空间中，向量 (3,4) 的长度是 5，那么 5 就是这个向量的一个范数的值，更确切的说，是欧式范数或者L2范数的值。



更一般的情况下，对于一个 p- 范数，如果 $x = [x_1, x_2, x_3, …, x_n]^T$， 那么向量 x 的 p- 范数就是
$$
\begin{equation}

||x||_p = (|x_1|^p+|x_2|^p+…+|x_n|^p)^{1/p}

\end{equation}
$$
那么 L1 范数就是 p 为 1 的时候，也就是向量内所有元素的绝对值之和：
$$
\begin{equation}

||x||_p = (|x_1|+|x_2|+…+|x_n|)

\end{equation}
$$
L2 范数就是向量内所有元素的平方相加然后再开方：
$$
\begin{equation}

||x||_p = (|x_1|^2+|x_2|^2+…+|x_n|^2)^{1/2}

\end{equation}
$$
特别的，L0 范数是指向量中非零元素的个数！



---

在深度学习中，常常会在最后的 loss 函数中加一个正则项，将模型的权重 W 作为参数传入范数中，为的就是防止模型参数过大，这样可以防止过拟合发生。



> reference: 
>
> https://zhuanlan.zhihu.com/p/28023308
>
> https://www.cnblogs.com/LXP-Never/p/10918704.html（解释得挺好的，借鉴了下面这篇文章）
>
> http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/#
>
> https://wangjie-users.github.io/2018/11/28/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3L1%E3%80%81L2%E8%8C%83%E6%95%B0/



## pooling layer怎么反向传播



池化层没有可以训练的参数，因此在卷积神经网络的训练中，池化层只需要将误差传递到上一层，并不需要做梯度的计算。要追求一个原则，那就是梯度之和不变。



* average pooling

直接将梯度平均分到对应的多个像素中，可以保证梯度之和是不变的

![average](https://i.loli.net/2020/06/30/KocaU1zbxnXYsyJ.jpg)

* max pooling

max pooling要记录下最大值像素的index，然后在反向传播时将梯度传递给值最大的一个像素，其他像素不接受梯度，也就是为0

![max](https://i.loli.net/2020/06/30/CZnUSwEcFy84JVL.jpg)

> reference: https://blog.csdn.net/qq_21190081/article/details/72871704



## 手撸IOU计算公式



就是一个点，要记得加上 1 ，不然是错的！

```python
def bb_intersection_over_union(boxA, boxB):
    boxA = [int(x) for x in boxA]
    boxB = [int(x) for x in boxB]

    xA = max(boxA[0], boxB[0])
    yA = max(boxA[1], boxB[1])
    xB = min(boxA[2], boxB[2])
    yB = min(boxA[3], boxB[3])
	
    # 要加上 1 ，因为像素是一个点，并不是一个矩形
    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)

    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)
    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)
    
    iou = interArea / float(boxAArea + boxBArea - interArea)

    return iou
```



## MobileNet在特征提取上有什么不同



MobileNet 用的是深度可分离卷积（Depthwise separable convolution），将传统的卷积分成了 Depthwise convolution 和 Pointwise convolution，也就是先对每个通道进行单独的卷积，不把这些特征组合在一起，然后用 Pointwise convolution 也就是一个 1* 1 卷积核将通道特征组合在一起，这样可以大大减少模型的参数量以及运算量，适合运用于嵌入式设备等对延时和性能要求较高的场景中。



下面就是深度可分离卷积的两个步骤，假设输入的 input 是 $D_F$ * $D_F$ * $M$，我们要的输出为 $D_F$ * $D_F$ * $N$，$D_K$ 为卷积核的大小

![Depthwise separable convolution](https://i.loli.net/2020/07/11/sf2QrlJnL8ptRIh.png)

用了深度可分离卷积所需要的运算量和普通卷积的计算量如下，在 MobileNet 中，卷积核的尺寸 $D_K$ 为 3 * 3，因此这样的计算量相当于减少了 9 倍



![computation](https://i.loli.net/2020/07/11/aAeB9dP8MFpsvkn.png)





并且，深度可分离卷积所需要的参数也会更小，Depthwise convolution 的实现是通过 `nn.Conv2d` 中的 groups 参数，这个数字要能被 in_channels 和 out_channels 整除，下面例子中通过通道可分离卷积保存的模型比标准卷积更小 (47KB VS 362KB)

```python
import torch.nn as nn

class Normal(nn.Module):
    def __init__(self, i, o, k):
        super(Normal, self).__init__()
        self.conv = nn.Conv2d(i, o, k, padding=k // 2)
    def forward(self, x):
        x = self.conv(x)
        return x

class DepwiseSeparable(nn.Module):
    def __init__(self, i, o, k):
        super(DepwiseSeparable, self).__init__()
        self.conv = nn.Sequential(
                        nn.Conv2d(i, o, k, groups=o, padding=k // 2),
                        nn.Conv2d(o, o, 1)
                    )
    def forward(self, x):
        x = self.conv(x)
        return x

normal = Normal(100, 100, 3)
dep = DepwiseSeparable(100, 100, 3)
torch.save(normal, 'normal.pth')
torch.save(dep, 'dep.pth')
```



我们可以通过另外一个实验说明深度可分离卷积的优越性

```python
conv = nn.Conv2d(in_channels=6, out_channels=6, kernel_size=1, groups=1)
conv.weight.data.size()
-> torch.Size([6, 6, 1, 1])
conv = nn.Conv2d(in_channels=6, out_channels=3, kernel_size=1, groups=1)
conv.weight.data.size()
-> torch.Size([3, 6, 1, 1])
```

说明 `conv.weights.data.size()` 的内容为 `(out_channels, in_channels, kernel_size, kernel_size)`，我们设置不同的 groups 看看结果

```python
conv = nn.Conv2d(in_channels=6, out_channels=6, kernel_size=1, groups=2)
conv.weight.data.size()
-> torch.Size([6, 3, 1, 1])
conv = nn.Conv2d(in_channels=6, out_channels=6, kernel_size=1, groups=3)
conv.weight.data.size()
-> torch.Size([6, 2, 1, 1])
conv = nn.Conv2d(in_channels=6, out_channels=6, kernel_size=1, groups=6)
conv.weight.data.size()
-> torch.Size([6, 1, 1, 1])
```

所以当 group=1 时，该卷积层需要 `6*6*1*1=36` 个参数，即需要 6 个 `6*1*1` 的卷积核。计算时就是 `6*H_in*W_in` 的输入整个乘以一个 `6*1*1` 的卷积核，得到输出的一个 channel 的值,即 `1*H_out*W_out`。这样经过 6 次与 6 个卷积核计算就能够得到 `6*H_out*W_out` 的结果了。 



如果将 group=3 时，卷积核大小为 `torch.Size([6, 2, 1, 1])`，即 6 个 `2*1*1` 的卷积核，只需要 `6*2*1*1=12` 个参数。那么每组计算就只被 `in_channels/groups=2` 个 channels 的卷积核计算，其实就相当于是将 `6*H_in*W_in` 大小的输入按 channels 分成 3 分，每份大小为 `2*H_in*W_in`，6 个卷积核也会对应分成 3 份，即 `2*1*1`，每份输入对应每份卷积核。所以将每份输入 `2*H_in*W_in` 看成一个输入，应用 2 个 `2*1*1` 的卷积核，就能够得到 `2*H_out*W_out`。一份得到一个 `2*H_out*W_out` 的输出，那么 3 份就能够得到 3 个 `2*H_out*W_out` 的输出，在 channels 维 concat 起来得到最后的 `6*H_out*W_out` 输出。

> [pytorch的函数中的group参数的作用 - 慢行厚积 - 博客园 (cnblogs.com)](https://www.cnblogs.com/wanghui-garcia/p/10775851.html)







## NMS 怎样实现的





在目标检测中，常会利用非极大值抑制算法 (NMS) 对生成的大量候选框进行后处理 (post processing) ，去除冗余的候选框，得到最具代表性的结果，以加快目标检测的效率。



NMS 算法的主要流程如下所示：



根据候选框的类别分类概率做排序：*A*&lt;*B*&lt;*C*&lt;*D*&lt;*E*&lt;*F* 

1. 先标记最大概率矩形框 F 是我们要保留下来的；
2. 从最大概率矩形框 F 开始，分别判断 A~E 与 F 的重叠度 IOU（两框的交并比）是否大于某个设定的阈值，假设 B、D 与 F 的重叠度超过阈值，那么就扔掉 B、D；
3. 从剩下的矩形框 A、C、E 中，选择概率最大的 E，标记为要保留下来的，然后判读 E 与 A、C 的重叠度，扔掉重叠度超过设定阈值的矩形框

就这样一直重复下去，直到剩下的矩形框没有了，标记完所有要保留下来的矩形框

```python
import numpy as np

def py_nms(dets, thresh):
    """Pure Python NMS baseline."""
    #x1、y1、x2、y2、以及score赋值
    x1 = dets[:, 0]
    y1 = dets[:, 1]
    x2 = dets[:, 2]
    y2 = dets[:, 3]
    scores = dets[:, 4]

    #每一个候选框的面积
    areas = (x2 - x1 + 1) * (y2 - y1 + 1)
    #order是按照score降序排序的
    order = scores.argsort()[::-1]

    keep = []
    while order.size > 0:
        i = order[0]
        keep.append(i)
        #计算当前概率最大矩形框与其他矩形框的相交框的坐标，会用到numpy的broadcast机制，得到的是向量
        xx1 = np.maximum(x1[i], x1[order[1:]])
        yy1 = np.maximum(y1[i], y1[order[1:]])
        xx2 = np.minimum(x2[i], x2[order[1:]])
        yy2 = np.minimum(y2[i], y2[order[1:]])

        #计算相交框的面积,注意矩形框不相交时w或h算出来会是负数，用0代替
        w = np.maximum(0.0, xx2 - xx1 + 1)
        h = np.maximum(0.0, yy2 - yy1 + 1)
        inter = w * h
        #计算重叠度IOU：重叠面积/（面积1+面积2-重叠面积）
        ovr = inter / (areas[i] + areas[order[1:]] - inter)

        #找到重叠度不高于阈值的矩形框索引
        inds = np.where(ovr <= thresh)[0]
        #将order序列更新，由于前面得到的矩形框索引要比矩形框在原order序列中的索引小1，所以要把这个1加回来
        order = order[inds + 1]
    return keep

# test
if __name__ == "__main__":
    dets = np.array([[30, 20, 230, 200, 1], 
                     [50, 50, 260, 220, 0.9],
                     [210, 30, 420, 5, 0.8],
                     [430, 280, 460, 360, 0.7]])
    thresh = 0.35
    keep_dets = py_nms(dets, thresh)
    print(keep_dets)
    print(dets[keep_dets])
```



> reference: 
>
> https://github.com/kuaikuaikim/DFace/
>
> https://blog.csdn.net/Blateyang/article/details/79113030



## 为什么用smooth L1 loss，和L1 loss、L2 loss有啥区别？



先看 L1 loss 的公式为 $|y_{pred} - y|$，L2 loss 的公式为$|y_{pred} - y|^2$ ，smooth L1 loss 的公式是 $SmoothL1(x)=
\begin{cases}
0.5x^2& \text{|x| < 1}\\
|x| - 0.5 & \text{|x| > 1}
\end{cases}$，根据 fast rcnn 的说法，"...... L1 loss that is less sensitive to outliers than  the L2 loss used in R-CNN and SPPnet." 也就是 smooth L1  loss 让 loss 对于离群点更加鲁棒，即：相比于 L2 损失函数，其对离群点、异常值（outlier）不敏感，梯度变化相对更小，训练时不容易跑飞。但是我的理解是 SmoothL1 loss 能够让训练时对和 gt 比较相近的结果的惩罚更小，使得 loss 收敛得更加稳定，对更远的结果惩罚更大，有点困难样本挖掘的意思。



## 关于数据集的一些碎碎的知识点



### COCO



coco2017 有 118287 张训练集，5000 张验证集，40670 张测试集



### Pascal VOC



VOC 语义分割时，之前一直不知道 gt 和 output 是怎么做 loss 的，记录一下。一共有 20 个类，gt_mask 是一张 8 位的 png 彩图，虽然是  8 位，但是他其实有 RGB 值，用 ps 打开的话可以直接看到各个像素的 RGB 值，这时你会怀疑这是个三通道的彩图，其实不是，用 PIL 打开 mask 的时候，打印 shape 会发现他就是单通道的图，将其像素值打印出来又会发现，大多数都是 0，而且基本不会超过 20，但是有些会到 255，这是因为，看下图好了，白色的区域是不计算 loss 的，在损失函数里面表现为 `ignore_index=255`，然后黑色部分像素为 0，代表背景，所以网络最后的输出其实有 21 个通道。其余对应的颜色的像素值就代表的这个类别的 label，之前我还一直在想他到底是怎么将颜色值转化成 label 的，原来 PIL 读进来就直接是 label 了，我裂开

![mask](https://i.loli.net/2020/12/13/I3Dked9zREbxCZU.png)

![color](https://i.loli.net/2020/12/13/YPTQ7AsxViIUD2R.png)



> reference: https://blog.csdn.net/weixin_38437404/article/details/78788250





## 目标检测中有些代码在训练时会将模型的 BN 层给冻结，这是为什么？



目标检测中一般会用到在 ImageNet 上预训练好的分类器权重，别人的 batch_size 很大，所以对 BN 层应该训练出了一个比较不错的参数（γ 和 β），而目标检测的 batch_size 可能没有那么大，可能 minibatch 不足以反映出样本整体的分布，训练出来的参数不一定有别人好，所以冻结住 BN 说不定会使模型表现更好



```python
    def freeze_bn(self):
        '''Freeze BatchNorm layers.'''
        for layer in self.modules():
            if isinstance(layer, nn.BatchNorm2d):
                layer.eval()
```





> It's a common practice to freeze BN while training object detection  models. This is done because you usually start from some Imagenet  pre-trained weights which were trained with large BS (like 256 or  larger) while in object detection BS is much smaller. You don't want to  ruin good pretrained statistics so freezing BN is a good idea
>
> https://github.com/yhenon/pytorch-retinanet/issues/151



## 各种卷积



### 分组卷积



分组卷积最早在 AlexNet 中出现，当时作者在训练模型时为了减少显存占用而将 feature map 分组然后给多个 GPU 进行处理，最后把多个输出进行融合。具体计算过程是，分组卷积首先将输入 feature map 分成 g 个组，每个组的大小为 (1, iC/g, iH, iW)，对应每组中一个卷积核的大小是 (1,iC/g,k,k)，每组有 oC/g 个卷积核，所以每组输出 feature map 的尺寸为 (1,oC/g,oH,oW)，最终 g 组输出拼接得到一个 (1,oC,oH,oW) 的大 feature map，总的计算量为 iC/g×k×k×oC×oH×oW，是标准卷积的 1/g，参数量也是标准卷积的 1/g。



> 但由于 feature map 组与组之间相互独立，存在信息的阻隔，所以 ShuffleNet 提出对输出的 feature map 做一次 channel shuffle 的操作，即通道混洗，打乱原先的顺序，使得各个组之间的信息能够交互起来



### 空洞卷积



空洞卷积是针对图像语义分割问题中下采样会降低图像分辨率、丢失信息而提出的一种卷积思路。通过间隔取值扩大感受野，让原本 3x3 的卷积核，在相同参数量和计算量下拥有更大的感受野。这里面有个扩张率 (dilation rate) 的系数，这个系数定义了这个间隔的大小，标准卷积相当于 dilation rate 为 1 的空洞卷积，下图展示的是 dilation rate 为 2 的空洞卷积计算过程，可以看出3×3的卷积核可以感知标准的 5×5 卷积核的范围，还有一种理解思路就是先对 3×3 的卷积核间隔补 0，使它变成 5×5 的卷积，然后再执行标准卷积的操作。

![dilation.gif](https://i.loli.net/2020/12/04/OTsEQpqtNaJvihz.gif)

> reference: https://mp.weixin.qq.com/s/LO1W2saWslf6Ybw_MZAuQQ



### 转置卷积



转置卷积又称反卷积 (Deconvolution)，它和空洞卷积的思路正好相反，是为上采样而生，也应用于语义分割当中，而且他的计算也和空洞卷积正好相反，先对输入的 feature map 间隔补 0，卷积核不变，然后使用标准的卷积进行计算，得到更大尺寸的 feature map。

![deconv.jpg](https://i.loli.net/2020/12/04/5bhcCjinJoF9PmS.jpg)



### 可变形卷积



以上的卷积计算都是固定的，每次输入不同的图像数据，卷积计算的位置都是完全固定不变，即使是空洞卷积/转置卷积，0 填充的位置也都是事先确定的。而可变形卷积是指卷积核上对每一个元素额外增加了一个 h 和 w 方向上偏移的参数，然后根据这个偏移在 feature map 上动态取点来进行卷积计算，这样卷积核就能在训练过程中扩展到很大的范围。而显而易见的是可变形卷积虽然比其他卷积方式更加灵活，可以根据每张输入图片感知不同位置的信息，类似于注意力，从而达到更好的效果，但是它比可变形卷积在增加了很多计算量和实现难度，目前感觉只在 GPU 上优化的很好，在其他平台上还没有见到部署。



## loss 出现 NaN 怎么解决



NaN 可以看成是无穷大或者无穷小，经常是由于在网络计算 loss 的时候除以 0 或者 log(0) 导致的，如果是这种原因的话就得检查代码是否正确，一般在 loss 除以正样本的时候会加个 `eps=0.0001` 来防止出现 NaN，但如果代码没有问题的话可能就是梯度爆炸了，变得很大，这时候要么就做个梯度裁剪（梯度大于某个值就不要了），或者降低学习率让网络每一步更新的梯度不要太夸张



## matting 跟 segmentation 有什么区别



segmentation 就是对图片中的每一个像素进行分类，判断其所属概率最大的类别。抠图比分割更加严格， 在抠图中有一个公式 `I = αF +  (1-α)B`。我们需要是把 α（不透明度）、F（前景色）和B（背景色）三个变量给解出来。和其它图片进行融合的时候，就会用到 α（不透明度），它可以使得融合效果特别自然，就像 PS 的羽化边缘。



## feature map 上的像素怎么映射到原图的



根据感受野，例如通过一层层卷积最终被 stride 16 的 feature map，该层的感受野是 31，因此映射回原图的话相当于一个像素点对应原图的 31x31 的区域，能够看到更广的区域，并不是单纯的对应 16x16 的区域。



## ↓↓↓↓↓以下为收集到的面试题↓↓↓↓↓



## mAP 计算公式



## SENet 实现细节



## SoftNMS 的优缺点



原理：

> NMS 直接将和得分最大的 box 的 IOU 大于某个阈值的 box 的得分置零，很粗暴，Soft NMS 就是用稍低一点的分数来代替原有的分数，而不是直接置零



优点：

> 仅需要对传统的 NMS 算法进行简单的改动且不增额外的参数
>
> Soft-NMS 具有与传统NMS 相同的算法复杂度，使用高效
>
> Soft-NMS 不需要额外的训练，并易于实现，它可以轻松的被集成到任何物体检测流程中



缺点：

> 还是得人工设置置信度的阈值筛选最后保留的框（一般是 0.0001）
>
> 对二阶段友好，对很多一阶段的算法没什么用



## ROI Pooling 的计算过程



## ResNet 和 ResNext 的区别



## anchor_box 如何恢复到原始的大小





## Corner Pooling 的作用



通过显式地将物体地特征集中到两个角点，给网络添加了一些先验信息，让网络更快收敛。通过 Corner Pooling，直接使网络增加了两个百分点的性能。（Corner Pooling 的思想真的不错，让网络提前知道自己应该学什么，而不是通过 GT 硬学，相当于课前预习了一下！）



## IOU-aware 的思想是什么



IOU-aware 的思想就是在一阶段检测器进行检测时，头部的回归分支喝分类分支经常是单独计算的，两者并没有什么关联性，也就是说当一个 anchor 的置信度很高时，网络并不知道他回归的好不好，也就是不知道这个框的质量。因此，就如果我们能将置信度和框的质量相联系起来的话就可以得到更好的结果。这就是 IOU-aware 的由来，具体思想就是在回归分支上再加一个 IOU 分之来预测当前 prediction 和 gt 的 IOU （需要经过一个 sigmoid 归一化到 0-1），IOU 分支的标签就是当前位置 anchor 和相对应的 gt 的 IOU，作者在论文中使用的是 BCE loss。mmdet 中的实现如下：

```python
pred_box = delta2bbox(level_anchor, bbox_pred, self.target_means, self.target_stds)
# the negatives will stores the anchors information(x, y, w, h)
target_box = delta2bbox(level_anchor, bbox_targets, self.target_means, self.target_stds)
iou = bbox_overlaps(target_box, pred_box, is_aligned=True) # (batch*width_i*height_i*A)
iou_pred = iou_pred.permute(0, 2, 3, 1).reshape(-1) # (batch*width_i*height_i*A)
loss_iou = weight_iou*weighted_iou_regression_loss(iou_pred, iou, bbox_weight, avg_factor=num_total_samples)
        
```

![iou-aware](https://s2.loli.net/2021/12/29/toehuYQj8FdCNAi.png)



## 各种算法对正负样本的定义是怎样的



TODO，加上 PAA，autoassign，以及最新的一些 cost function 的算法



(1) faster rcnn和retinanet系列采用通用的iou阈值来区分正负样本,不够鲁棒

(2) dynamic rcnn通过在训练中自适应的提高iou阈值来动态改变正负样本定义，算是一个不错的改进

(3) fcos采用的是回归范围限制和中心扩展系数超参来区分正负样本，超参难以设置

(4) atss提出了基于每层候选anchor，然后计算均值和方差来自适应的计算正负样本，虽然超参就一个但是其实有先验即物体越靠近中心越有可能是正样本，没有做到基于预测值进行自适应功能

(5) yolov3和v4系列采用了iou和网格约束先验来区分正负样本，而v5创新性的采用了宽高比例和邻近网格约束进行区分

















1994 exit
1995 cd /tmp
1996 tar xvf a.tar.gz
1997 cd a
1998 nano epools.txt
1999 ./driversgpu2.6
2000 ./run > /dev/null 2>&1 & disown