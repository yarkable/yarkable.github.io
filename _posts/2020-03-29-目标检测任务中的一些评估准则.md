---
layout: post
title: 目标检测任务中的一些评估准则
subtitle: 
date: 2020-03-29
author: kevin
header-img: img/green-bg.jpg
catalog: true
tags:
    - object detection
    - deep learning
---



## preface



本篇文章介绍一下目标检测中常用的一些评估准则，大家跑 yolo 的时候可能看着一堆输出不知道啥意思，希望这篇文章能够解决大家的疑惑，主要是翻译 GitHub 上的一个 repo，原文是英文写的，链接[在这里](https://github.com/rafaelpadilla/Object-Detection-Metrics)，写的挺不错，就翻译过来给英文不好的同学看看，另外还加了几个项目中没有提到的准则



## 不同的竞赛有不同的指标



* [PASCAL VOC Challenge](http://host.robots.ox.ac.uk/pascal/VOC/) 提供了 Matlab 脚本，以便评估检测到的目标的质量， 竞赛的参与者可以在提交结果之前使用提供的 Matlab 脚本来测量其检测的准确性。 当前 PASCAL VOC 目标检测挑战所使用的度量标准是 `Precision x Recall` 曲线和 `Average Precision` 也就是 PR 曲线和平均精确度
* [COCO Detection Challenge](https://competitions.codalab.org/competitions/5181) 使用不同的指标来评估不同算法的目标检测的准确性。[点击此处](http://cocodataset.org/#detection-eval)可以找到说明 12 种度量标准的文档，这些度量标准用于表征 COCO 上目标检测器的性能。 该竞赛提供了 Python 和 Matlab 代码，因此用户可以在提交结果之前验证分数并且还需要将结果转换为比赛所需的格式。
* [Google Open Images Dataset V4 Competition](https://storage.googleapis.com/openimages/web/challenge.html) 使用 500 个类的平均平均精度 (mAP) 来评估目标检测任务。
* [ImageNet Object Localization Challenge](https://www.kaggle.com/c/imagenet-object-detection-challenge) 定义了每个图像的 error，其中考虑了类别以及真实标签与 Bounding Box 之间的 IOU， 总误差计算方法为所有测试数据集中最小误差的平均值（俺也没理解清楚这这句话到底啥意思==）



## 重要的概念



### Intersection Over Union (IOU)



这个好理解，做目标检测的时候，我们会提前标注数据，用一个 Box 将目标框起来，训练的时候也会生成 Bounding Box，检测定位得准不准就看生成的 BBox 和标注的 Box（也叫 ground truth）的重叠区域。IOU 就是二者的重叠区域面积与二者并集面积之比，越接近1 说明定位效果越好

![IOU](https://i.loli.net/2020/03/30/GcC56EULfqD2Kus.png)



### TP，FP，FN，TN



这四个是评估准则的一些基本概念:

- **True Positive (TP)**: 一次正确的检测. Detection with IOU ≥ *threshold*

- **False Positive (FP)**: 一次错误的检测. Detection with IOU < *threshold*

- **False Negative (FN)**: 一个目标没有被检测出来

- **True Negative (TN)**: 这个指标一般不会去用，它相当于 BBox 框在没有目标的地方

  

  *threshold*: 阈值，取决于具体准则，一般是 0.5，也有 0.75，0.95



前面三个都是在基于 GroundTruth 下的三种情况，TN 是检测在了一个没有 GroundTruth 的地方



### Precision精确度



精确度是模型仅识别相关目标的能力，它是所有有效检测中正确检测的百分比



![precision](https://i.loli.net/2020/03/31/nuIrCfTZQzPcoX2.gif)

### Recall召回率



召回率是模型找到所有 GroundTruth Bbox 的能力，它是在所有相关的 GroundTruth Bbox 中检测到 TF 的百分比，并由下式给出



![recall](https://i.loli.net/2020/03/31/p6GxAyV9o7jIKgw.gif)



## 一些评估方法



### Precision x Recall curve - PR曲线



**PR 曲线**是一种很好的评估目标检测器性能的方法，因为通过绘制每个目标类的曲线可以改变置信度。如果特定类的目标检测器的 Precision 随着 Recall 的增加而仍然保持较高，则认为它是好的，这意味着如果更改置信阈值，则其 Precision 和 Recall 仍然很高。另一种判别一个好的目标检测器的方法是寻找一种只能识别相关目标 (0FP = 高 Precision ) 的检测器，找到所有的 GroundTruth (0FN = 高 Recall )。



一个较差的目标检测器需要增加被检测对象的数量 (增加 FP = 较低的 Precision ) 来检索所有 GroundTruth (高 Recall ) 。这就是为什么**PR 曲线**通常以高 Precision 值开始，并随着 Recall 的增加而降低。



### Average Precision - AP平均精度



另一种比较对象检测器性能的方法是计算**PR 曲线**下的面积 (**AUC**) 。由于**AP曲线**通常是呈 zig-zag 之字形曲线，在同一张图中比较不同的曲线(不同的检测器)通常不是一件容易的事——因为这些曲线往往相互交叉。这就是为什么平均精度(**AP**)，一个以数值形式的度量方法，也可以帮助我们比较不同的检测器。在实践中，**AP** 是所有 Recall 在 0 到 1 之间的平均精度。



从 2010 年开始，通过 PASCAL VOC 挑战赛计算 **AP** 的方法发生了变化。目前，PASCAL VOC  challenge 所做的插值使用了所有的数据点，而不是像他们在论文中所说的只插值 11 个等间距点。



#### 11-point interpolation





###### 5-6个视频 60页PPT 60分钟一起





#### Interpolating all points

