---
layout: post
title: Linux服务器管理员操作
subtitle: 
date: 2021-04-16
author: kevin
header-img: img/green-bg.jpg
catalog: true
tags:
    - linux
---



## 添加用户



由于每台服务器都需要连接到 NAS，而且可能很多用户在不同的服务器上都有账号，这样的话就会导致 uid 冲突（不同服务器上不同用户的 uid 可能是一样的），因此，针对不同情况需要用到不同添加用户的方法：

1. 该用户为新同学，说明他之前在其他服务器上没有账号，因此，先在 NAS 上为他开一个账号确保 uid 唯一性，再根据这个 uid 去其他的服务器上进行开号
2. 该用户在其他服务器上有账号，那就直接根据他的 uid 进行开号，无需再经过一遍 NAS



开号方式使用命令 `useradd` ，默认情况下直接 `useradd user1` 就可以了，用户目录为 `/home/user1`，但是考虑到服务器硬盘容量有限，最好将其划分到具有更大空间的目录如 `/data`，因此使用如下命令进行自定义添加用户



```shell
$ useradd -u [uid] -d /data/user1 -m -s /bin/bash user1
```



| 选项        | 含义                                                         |
| ----------- | ------------------------------------------------------------ |
| -u UID      | 手工指定用户的 UID，注意 UID 的范围（不要小于 500）。        |
| -d 主目录   | 手工指定用户的主目录。主目录必须写绝对路径，而且如果需要手工指定主目录，则一定要注意权限； |
| -c 用户说明 | 手工指定/etc/passwd文件中各用户信息中第 5 个字段的描述性内容，可随意配置； |
| -g 组名     | 手工指定用户的初始组。一般以和用户名相同的组作为用户的初始组，在创建用户时会默认建立初始组。一旦手动指定，则系统将不会在创建此默认的初始组目录。 |
| -G 组名     | 指定用户的附加组。我们把用户加入其他组，一般都使用附加组；   |
| -s shell    | 手工指定用户的登录 Shell，默认是 /bin/bash；                 |
| -e 曰期     | 指定用户的失效曰期，格式为 "YYYY-MM-DD"。也就是 /etc/shadow 文件的第八个字段； |
| -o          | 允许创建的用户的 UID 相同。例如，执行 "useradd -u 0 -o usertest" 命令建立用户 usertest，它的 UID 和 root 用户的 UID 相同，都是 0； |
| -m          | 建立用户时强制建立用户的家目录。在建立系统用户时，该选项是默认的； |
| -r          | 创建系统用户，也就是 UID 在 1~499 之间，供系统程序使用的用户。由于系统用户主要用于运行系统所需服务的权限配置，因此系统用户的创建默认不会创建主目录。 |

> 表格引自 http://c.biancheng.net/view/844.html



## 更新 CUDA

先装 CUDA [[下载地址](https://developer.nvidia.com/zh-cn/cuda-toolkit)]，老版本的 CUDA 不用删掉，直接让管理员将 cuda 软连接到最新的 CUDA 就行了，以防有些代码需要低版本 CUDA

再装驱动 [[驱动下载地址]](https://www.nvidia.cn/Download/index.aspx?lang=cn)，安装过程会提示说检测到老版本驱动，直接卸载就行了





## 常用命令



| 命令                                  | command                                                      |
| ------------------------------------- | ------------------------------------------------------------ |
| 查看 GPU 使用状态                     | nvidia-smi 、 gpustat -i (需 pip install gpustat)            |
| 查看进程                              | top、htop、ps -ef \| grep [pid]                              |
| 查看服务器磁盘容量                    | df -h                                                        |
| 查看自己占用服务器的容量              | du -h                                                        |
| 查看当前目录下文件个数 (不包含子目录) | ls -l \| grep "^-" \| wc -l                                  |
| 查看端口占用 (Linux)                  | lsof -i:PORT (没有空格)                                      |
| 查看端口占用 (Windows)                | 查看所有开放端口： netstat -ano<br />查看占用端口程序的 PID：netstat -aon \| findstr "PORT"<br />查看占用端口的 PID 所对应的程序：tasklist \| findstr "PID"<br />杀死占用端口的进程：taskkill /T /F /PID "PID" |



## 换源



### pip 源



> vim ~/.pip/pip.conf

```txt
[global]

index-url = https://pypi.doubanio.com/simple

trusted-host = pypi.doubanio.com
```



### conda 源



> vim ~/.condarc

```
channels:
  - defaults
show_channel_urls: true
default_channels:
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2
custom_channels:
  conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
```



## 迁移 conda 环境



有时候我们需要在机器上重新建一个 conda 环境，但是又不想重新装包，毕竟 pytorch 和 cuda 版本都跟之前的环境是一样的，所以可以直接从之前的环境中复制一份成为新环境，conda 是支持这样做的，以下命令就将 BBB 环境拷贝了一份成为 AAA 环境。

```shell
conda create -n AAA --clone BBB
```

如果涉及不同服务器之间装环境的话也一样，可以先将旧的环境拷贝到新的电脑，然后通过下面的命令创一个新的环境

```shell
conda create -n AAA --clone ~/path 
```

可以使用 `conda info -e`  来查询机器上的所有 conda 环境以及对应所在的位置。





## 安装 anaconda 后默认用的是别人的环境



具体表现为我在 253 上面装完 anaconda 之后显示的 base 环境是师兄的，然后我能新建环境，但是我不能切换到我的环境，一直报错

```
mmdet                    /home/kevin/.conda/envs/mmdet
                         /home/kevin/anaconda3
base                  *  /home/whqsx/anaconda3
torch_1.6                /home/whqsx/anaconda3/envs/torch_1.6
```

```
kevin@LabServer:~$ conda activate mmdet

CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.
If your shell is Bash or a Bourne variant, enable conda for the current user with

    $ echo ". /home/whqsx/anaconda3/etc/profile.d/conda.sh" >> ~/.bashrc

or, for all users, enable conda with

    $ sudo ln -s /home/whqsx/anaconda3/etc/profile.d/conda.sh /etc/profile.d/conda.sh

The options above will permanently enable the 'conda' command, but they do NOT
put conda's base (root) environment on PATH.  To do so, run

    $ conda activate

in your terminal, or to put the base environment on PATH permanently, run

    $ echo "conda activate" >> ~/.bashrc

Previous to conda 4.4, the recommended way to activate conda was to modify PATH in
your ~/.bashrc file.  You should manually remove the line that looks like

    export PATH="/home/whqsx/anaconda3/bin:$PATH"

^^^ The above line should NO LONGER be in your ~/.bashrc file! ^^^ 
```



上官网仓库找报错问题，只需要用一行代码就可以解决这个问题

```bash
source ~/anaconda3/etc/profile.d/conda.sh
conda activate my_env
```





## 挂载 NAS



其实就是将 NAS 上的目录映射到本地一个目录，所以新建一个目录叫做 `/NAS_REMOTE` ，用 apt 先安装 `nfs-utils` ，在 `sudo vim /etc/fstab` 在最底下添加一行 (前面是被挂载的目录，后面是本地挂载目录)

```
172.31.233.218:/share/CACHEDEV1_DATA/Public /NAS_REMOTE nfs defaults 0 0

```



之后再运行 `sudo mount -a` ，就能将 NAS 挂载上，以后重启机器的话也要运行一下这个命令进行挂载



> 挂载其他服务器 https://cshihong.github.io/2018/10/16/NFS%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%90%AD%E5%BB%BA%E4%B8%8E%E9%85%8D%E7%BD%AE/

```bash
[root@localhost ~] yum install -y nfs-utils   
#安装nfs服务
[root@localhost ~] yum install -y rpcbind
#安装rpc服务
[root@localhost ~] systemctl start rpcbind    #先启动rpc服务
[root@localhost ~] systemctl enable rpcbind   #设置开机启动
[root@localhost ~] systemctl start nfs-server nfs-secure-server      
#启动nfs服务和nfs安全传输服务
[root@localhost ~] systemctl enable nfs-server nfs-secure-server
[root@localhost /] firewall-cmd --permanent --add-service=nfs
success   #配置防火墙放行nfs服务
[root@localhost /] firewall-cmd  --reload 
success
```



## 批量 kill 进程 



用 grep 配合 awk 可以轻易做到，`awk '{print $2}'` 表示输出第二列结果，在 ps 命令中就是进程的 id 号

```shell
ps -ef | grep xxx | grep -v grep | awk '{print $2}' | xargs kill -9
```



## 莫名其妙占用显存



有时候明明没人用卡，但是卡的显存却被占用了很多，也找不到卡上的进程，这是因为上一个用卡的人的程序退出了，但是又没完全退出，让这个用户输入下面命令就可以清空显存（ 里面的 X 是 GPU 的 id 号 ），不过要注意，可能导致该用户所有 GPU 进程全部被终结，所以最好让该用户在没有使用 GPU 的时候输入命令

```bash
fuser -v /dev/nvidiaX | awk "{print $2}" | xargs kill -9
```





## 86只能被233网段的机器连接



有时候重启了 233.86 之后，会出现 ssh 连接不上的情况，但是 233.xx 的 ip 可以连接上，这是因为 86 用的默认网卡是一张有问题的卡（不知道是谁设置的），默认走的是这张网卡，使用 `ip route` 命令可以看到，如果第一行的 default 不是连接到学校内网的网卡的话，就是有问题的，需要用 `ifconfig <网卡名> down` 把这块网卡关掉，然后再 `ip route` 查看，第一行 default 变了的话就是成功了



## 根目录列表无法显示



具体表现为，在根目录输入 `ls` 命令之后一直卡死，按 `CTRL+C` 都退不出去，没错，我说的就是 189 服务器。然后就输入 `df -h` 想看看服务器的磁盘使用情况，依然卡死，无法退出。于是查看一下本地的磁盘使用，输入 `df -hl` 有正常的输出，那就说明本地的文件系统没有问题，那么就可能是挂载了其他服务器上的磁盘，因为其他服务器出了问题，导致 `ls` 的时候一直在等待这个服务器的响应。于是输入 `mount` 查看服务器是否有挂载其他服务器文件夹，出来两个文件系统，一个是我们的 NAS，一个是师兄自己的 NAS，分别 ping 他们的 ip，都能 ping 通，所以不存在机器关机的问题，然后分别进入这两个文件夹，发现师兄的 NAS 可以正常进入，而我们的 NAS 进不去，并且一直卡着，那么问题找到了，我们的 NAS 出了问题。



怎么解决呢，一个好的办法就是将他取消挂载，但是用 `umount -f /NAS_REMOTE` 的话会说 `device is busy`，要用 `umount -l /NAS_REMOTE` ，这样就可以取消挂载，可以列出根目录列表了。随后检查一下 NAS，发现没有什么异常，在其他服务器取消挂载后再挂上去也一样正常，但是在 189 输入 `sudo mount -a` 重新挂载 NAS 后又会卡死，目前暂时未解决这个问题，估计重启后就可以了。



造成这种现象的原因是 nfs 服务器/网络挂了，nfs 客户端默认采用 hard-mount 选项，而不是 soft-mount。他们的区别是：

* soft-mount: 当客户端加载 NFS 不成功时，重试 retrans 设定的次数.如果 retrans 次都不成功，则放弃此操作，返回错误信息 "Connect time out"
* ard-mount: 当客户端加载 NFS 不成功时，一直重试，直到 NFS 服务器有响应。hard-mount 是系统的缺省值。
  

> reference：
>
> https://blog.csdn.net/Bronze5/article/details/79113378
>
> https://blog.csdn.net/qq_36270681/article/details/104408077
>
> https://blog.csdn.net/BrotherDong90/article/details/51735632



## A100 突然之间用不了GPU



经常在 A100 上发生，突然之间用不了 GPU 了，具体表现为，torch 的 `torch.cuda.is_available()` 为 False，并且运行英伟达的示例程序也 return 一个错误代码。花了些时间找出这是什么造成的，这是 A100 独有的问题，因为它需要一个 `fabricmanager` 服务，但是这个服务经常会突然崩掉，所以我们又得将这个服务重新安装，然后再 enable 它，GPU 就能够正常使用了。

```shell
distribution=$(. /etc/os-release;echo $ID$VERSION_ID | sed -e 's/\.//g') \
&& wget https://developer.download.nvidia.com/compute/cuda/repos/$distribution/x86_64/cuda-$distribution.pin \
&& sudo mv cuda-$distribution.pin /etc/apt/preferences.d/cuda-repository-pin-600

sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/$distribution/x86_64/7fa2af80.pub \
&& echo "deb http://developer.download.nvidia.com/compute/cuda/repos/$distribution/x86_64 /" | sudo tee /etc/apt/sources.list.d/cuda.list \
&& sudo apt-get update

sudo apt-cache madison cuda-drivers-fabricmanager-450

sudo apt-get install -y cuda-drivers-fabricmanager-450

reboot

systemctl start nvidia-fabricmanager.service
systemctl enable nvidia-fabricmanager.service
```



## 服务器频繁重启



很勾巴烦，服务器好端端的自己重启，具体表现为 ping 不到 ip，ssh 断连，一段时间开机后恢复。开始以为是 GPU 出了什么问题，没人用 GPU 就会重启，然后发现用了 GPU 还是重启，于是找客服远程看看，最终还是没解决，不过学到了一些查看 Linux 系统日志的知识，记录一下。



首先，`last` 命令可以输出之前登陆过系统的用户的名字和登录时长，并且可以看到系统重启的时间。最后三行表示的是登陆时间、退出时间以及持续时间，注意倒数第二行，如果是正常重启的话就是 `down`，电源强制重启的话就显示 `crash`，可以看到我们这里很多个 `crash`，所以系统可能有点问题。

```
lzr      pts/3        172.29.3.101     Tue Aug 31 11:46   still logged in
lzr      pts/2        172.29.3.101     Tue Aug 31 11:46   still logged in
lzr      pts/1        172.31.71.85     Tue Aug 31 11:33   still logged in
lzr      pts/0        172.29.3.101     Tue Aug 31 11:00   still logged in
reboot   system boot  3.10.0-1160.6.1. Tue Aug 31 18:56 - 20:13  (01:16)
lzr      pts/12       172.29.3.101     Tue Aug 31 10:53 - 10:54  (00:00)
lzr      pts/12       172.29.3.101     Tue Aug 31 10:53 - 10:53  (00:00)
lzr      pts/12       172.29.3.101     Tue Aug 31 10:53 - 10:53  (00:00)
lzr      pts/12       172.29.3.101     Tue Aug 31 10:53 - 10:53  (00:00)
lzr      pts/12       172.29.3.101     Tue Aug 31 10:53 - 10:53  (00:00)
lzr      pts/11       172.29.3.101     Tue Aug 31 10:47 - crash  (08:09)
lzr      pts/10       172.29.3.101     Tue Aug 31 09:43 - crash  (09:13)
lzr      pts/9        172.29.3.101     Tue Aug 31 03:10 - crash  (15:46)
lzr      pts/5        172.31.71.186    Mon Aug 30 21:39 - crash  (21:17)
lzr      pts/4        172.31.71.186    Mon Aug 30 21:37 - crash  (21:19)
lzr      pts/0        172.31.71.85     Mon Aug 30 21:13 - crash  (21:43)
reboot   system boot  3.10.0-1160.6.1. Tue Aug 31 05:03 - 20:13  (15:10)
lzr      pts/6        172.31.71.85     Mon Aug 30 16:55 - 16:55  (00:00)
lzr      pts/6        172.31.71.85     Mon Aug 30 16:55 - 16:55  (00:00)
lzr      pts/5        172.31.71.85     Mon Aug 30 14:38 - crash  (14:25)
guanhua_ pts/4        172.31.108.76    Mon Aug 30 14:02 - crash  (15:01)
lzr      pts/1        172.31.71.186    Mon Aug 30 10:41 - crash  (18:22)
lzr      pts/0        172.31.71.186    Mon Aug 30 10:40 - crash  (18:22)

```



知道系统有问题之后我们可以去查看系统日志，一般在 `/var/log/messages` 里面，ubuntu 的话在 `/val/log/syslog` 里面，这个文件记录了系统的一些情况，但是看着也没啥能跟重启有关的问题。

```
Aug 31 20:15:18 localhost systemd: sshd.service start operation timed out. Terminating.
Aug 31 20:15:18 localhost systemd: Failed to start OpenSSH server daemon.
Aug 31 20:15:18 localhost systemd: Unit sshd.service entered failed state.
Aug 31 20:15:18 localhost systemd: sshd.service failed.
Aug 31 20:15:26 localhost kernel: NFSD: client 172.31.234.159 testing state ID with incorrect client ID
Aug 31 20:15:26 localhost kernel: NFSD: client 172.31.234.159 testing state ID with incorrect client ID
Aug 31 20:16:00 localhost systemd: sshd.service holdoff time over, scheduling restart.
Aug 31 20:16:00 localhost systemd: Stopped OpenSSH server daemon.
Aug 31 20:16:00 localhost systemd: Starting OpenSSH server daemon...
Aug 31 20:16:01 localhost dbus[1413]: [system] Activating service name='org.fedoraproject.Setroubleshootd' (using servicehelper)
Aug 31 20:16:01 localhost systemd: Created slice User Slice of root.
Aug 31 20:16:01 localhost systemd: Started Session 633 of user root.
Aug 31 20:16:01 localhost systemd: Removed slice User Slice of root.
Aug 31 20:16:01 localhost dbus[1413]: [system] Successfully activated service 'org.fedoraproject.Setroubleshootd'
Aug 31 20:16:02 localhost setroubleshoot: failed to retrieve rpm info for /usr/local/lib/libnss-2.1.2.so

```



因为我们这种情况可能是系统崩溃，所以还可以去查看系统崩溃报告，在 `/var/crash/xxx` 里面有系统崩溃时的内核报告，给出了崩溃时的 core dump，这东西得专业的人来看才看得懂。

```
[49827.892064] Call Trace:
[49827.892891]  <IRQ>
[49827.893018]  [<ffffffffc1a1d15c>] ? os_get_current_tick+0x2c/0x60 [nvidia]
[49827.894890]  [<ffffffffc1e7cd3c>] ? _nv035997rm+0x2c/0x90 [nvidia]
[49827.895801]  [<ffffffffc1a4e761>] ? _nv009219rm+0x6d1/0x710 [nvidia]
[49827.896702]  [<ffffffffc1a4f57c>] ? _nv036101rm+0x2c/0x120 [nvidia]
[49827.897638]  [<ffffffffc1a98883>] ? _nv032953rm+0x33/0x1a0 [nvidia]
[49827.898519]  [<ffffffffc1a0d470>] ? nvidia_frontend_ioctl+0x40/0x40 [nvidia]
[49827.899451]  [<ffffffffc22d84e6>] ? rm_run_rc_callback+0x86/0xd0 [nvidia]
[49827.900332]  [<ffffffffc1a0dfdc>] ? nvidia_rc_timer_callback+0x3c/0x60 [nvidia]
[49827.901210]  [<ffffffffc1a0d47d>] ? nv_timer_callback_typed_data+0xd/0x10 [nvidia]
[49827.902003]  [<ffffffffac8abd58>] ? call_timer_fn+0x38/0x110
[49827.902868]  [<ffffffffc1a0d470>] ? nvidia_frontend_ioctl+0x40/0x40 [nvidia]
[49827.903660]  [<ffffffffac8ae1ed>] ? run_timer_softirq+0x24d/0x300
[49827.904438]  [<ffffffffac8a4b95>] ? __do_softirq+0xf5/0x280
[49827.905199]  [<ffffffffacf984ec>] ? call_softirq+0x1c/0x30
[49827.905954]  [<ffffffffac82f715>] ? do_softirq+0x65/0xa0
[49827.906709]  [<ffffffffac8a4f15>] ? irq_exit+0x105/0x110
[49827.907447]  [<ffffffffacf99a88>] ? smp_apic_timer_interrupt+0x48/0x60
[49827.908171]  [<ffffffffacf95fba>] ? apic_timer_interrupt+0x16a/0x170
[49827.908876]  <EOI>
```



在知道服务器是由于崩溃导致重启之前，我怀疑过系统是不是被黑客入侵了然后留了个定时关机脚本，我查看 `crontab` 里面发现没有，然后我又觉着网络上行流量好大，是不是黑客留下了某个程序在一直往外面发送文件，于是我查询有没有根据进程名显示流量占用量的程序，然后找到了 `nethogs` 这个工具，最终发现，这么大的流量是因为我将服务器的一个目录映射成了 NFS 服务器，它在和客户端进行数据传输。



查询了 google 上的相关问题，很少有人出现我这种情况，但是出现这种情况的都是电源出了问题，所以我猜测也应该是电源出了问题，要么就是机房电压不稳。打电话让工程师过来机房看了一下，发现有根电源线已经快掉了，然后给他插紧了，应该是这个原因导致的。



## 给服务器加硬盘



TODO，还没有试过

[Linux服务器增加硬盘操作记录 | Yunfeng's Simple Blog (vra.github.io)](https://vra.github.io/2017/02/24/linux-add-disk/)



## PyCharm无法列出目录



有一天，skh 的电脑莫名其妙出现这个问题，网上查都查不到是什么原因

```
[2021/9/17 23:26] Upload to kevin@172.31.233.142:22 (3)
[2021/9/17 23:28] Upload to kevin@172.31.233.142:22 (3) failed: could not list the contents of folder "sftp://172.31.233.142/". (Timeout expired)
[2021/9/18 21:17] Upload to kevin@172.31.233.142:22 (3)
[2021/9/18 21:19] Upload to kevin@172.31.233.142:22 (3) failed: could not list the contents of folder "sftp://172.31.233.142/". (Timeout expired)
```



我想起上次师妹好像也有这个问题，也没解决，然后众人和 skh 搞了一晚上，尝试过卸载 PyCharm，未果，就差重装系统了。大伙们都以为是 skh 的 PyCharm 有问题，他本人也如是觉着，走前我用我的 PyCharm 上传文件到服务器，结果发现也出了这个错误，竟然出现了机传机现象！而且用 PyCharm 的都中招了！起初人们以为这只是一个小 bug，直到自己的 PyCharm 也打不开了才不得不重视起来，连夜找 VSCode 的替代方案，结果发现 VSCode 还确实能够做到一份代码同步到多个服务器上，这个我以后专门写一份教程。第二天，在我和 jlchen 讨论出一个曲线救国的 VSCode 环境配置之前，我觉得 VSCode 多台服务器远程开发还是有缺点，还是 PyCharm 更香，于是我又去找了上面那个报错的原因，其中一份解决方案里面提到了根目录，如果要将 remote 资源列出来的话，要从根目录列，这时候我突然想起来！我有台服务器用作 NAS 存数据了，所以很多服务器上都挂了我的这个 NAS，但是我这台 NAS 在昨天下午被意外关机了，一直没有开机。上次我在 189 上就有这个问题，最后把挂载的服务器取消挂载就行了，没想到这次可能又是这个问题，只不过换了种形式，我马上去实验室服务器上 `ls /` 列一下根目录，果然，卡住了！这波不是 PyCharm 的锅，是我的锅，把 NAS 卸载就行了，继续拥抱 PyCharm。



## 重启机器后硬盘丢失



有时机器重启或者关机后，再次开机就找不到某块硬盘了，这时候我们需要手动去挂载他，找到丢失的硬盘的位置，然后再挂载，主要是找到硬盘编号比较重要，一般是 sdb，sda 之类的代号，可以通过 `sudo fdisk -l` 查找，如果是未挂载的硬盘是查看不到里面的文件的。

```shell
sudo mount /dev/sdb /mnt/nas/
```





## 查看内存占用



输入 `ps aux | sort -k4nr | head -n 5` 查看占用内存最多的前 5 个进程，或者也可以通过 `top` 命令后按住 `M` 来对内存占用进行排序，两个都可以。利用 `ps -aux` 或者 `top` 命令也可以查看到具体的占用多少 G 内存，举个例子，这是 `top` 命令的界面，`%MEM` 就是内存的占用量，对 `250508` 这个进程来分析一下，它的占用率是 2.1%，我们服务器内存大概是 504G，得出这个进程占用了大约 10.6G 的内存

```bash
   PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
250508 wubizhu   20   0 92.497g 0.010t  88056 S   0.0  2.1   0:32.10 python
250593 wubizhu   20   0 92.498g 0.010t  87900 S   0.0  2.1   0:29.64 python
225884 xjheng    20   0 49.620g 0.010t 6.367g S   4.8  2.0  57:06.42 python
 83813 xjheng    20   0 52.932g 6.760g 3.045g R 101.9  1.3 205:04.84 python
252689 zxdong    20   0 14.935g 4.106g  83604 D   2.6  0.8  22:00.05 python
252681 zxdong    20   0 14.935g 4.106g  83608 D  30.4  0.8  22:18.04 python
 13646 xjheng    20   0 17.928g 4.022g 1.127g D  22.7  0.8   1:29.93 python
```

验证一下说法，通过 `ps -aux | grep 250508` 得到下面结果，第六列 `10944660` 就是占用的物理内存，单位是 k，所以这里统计出的是 10.9G，跟我们算出来的差不多

```bash
wubizhu  250508  1.8  2.0 96990572 10944660 pts/131 Sl+ 11:50   0:42 python xx.py
```

通过 `cat /proc/250508/status` 也能得到进程的内存占用量，`VmRSS` 就是物理内存使用量，单位也是 k

```bash
Name:   python
Umask:  0002
State:  S (sleeping)
Tgid:   250508
Ngid:   250508
Pid:    250508
PPid:   38642
TracerPid:      0
Uid:    1074    1074    1074    1074
Gid:    1074    1074    1074    1074
FDSize: 128
Groups: 1074
NStgid: 250508
NSpid:  250508
NSpgid: 38642
NSsid:  9950
VmPeak: 97005580 kB
VmSize: 96990572 kB
VmLck:         0 kB
VmPin:         0 kB
VmHWM:  10961980 kB
VmRSS:  10944656 kB

```



## BMC 相关 （TODO）





> BMC是一个独立于服务器系统的小型操作系统，作用是方便服务器远程管理、监控、安装、重启等操作。BMC接通电源即启动运行，由于独立于业务程序不受影响，避免了因死机或者重新安装系统而进入机房。
>
> BMC只是一个集成在主板上的芯片（也有通过PCIE等各种形式插在主板上），对外表现形式只有一个标准RJ45网口，拥有独立IP。普通维护只需使用浏览器访问IP:PORT登录管理页面，服务器集群一般使用BMC指令进行大规模无人值守操作。
>
> 一般服务器BMC网口是独立的，仔细看印有BMC字样。但是也有小型服务器BMC网口和通信网口是二合一的。
>
> 当然也有不叫BMC的，只要遵守IPMI协议，都是类似的。





## 显卡监控



因为之前服务器上经常有人一下就用很多卡，导致别人没卡跑，所以我写了个监控脚本在我管理的服务器上，每个人最多能占用两张卡的显存，超过的话就将程序给 kill 了，维护一个良好的秩序。

```python
import os
import re
import time

users = 'root,florence,jeffin,jinziqi,jlchen,kevin,lmm,luocheng,weizeng,wubizhu,xiaox,xjheng,yuxuan,zsting,zxdong'.split(',')

def execCmd(cmd):
    r = os.popen(cmd)
    text = r.read()
    r.close()
    return text

while True:
    info = execCmd('gpustat -cpu | grep python | awk \'{for(i=13;i<=NF;i++) printf $i""FS;print ""}\'')
    pc_info = info.split('\n')
    pc_info = [x for x in pc_info if x != '']

    user_dict = dict([(user.strip(), dict()) for user in users ])
    for i, mem_info in enumerate(pc_info):
        one_card_info = mem_info.split(' ')
        one_card_info = [x for x in one_card_info if x != '']

        for val in one_card_info:
            user = val[:re.search(':', val).span()[0]]
            gpu_mem = val[re.search('\(', val).span()[1]: -2]
            pid = val[re.search('/', val).span()[1] : re.search('\(', val).span()[0]]
            if pid in user_dict[user].keys():
                user_dict[user][pid] += int(gpu_mem)
            else:
                user_dict[user][pid] = int(gpu_mem)

    for user, info in user_dict.items():
        user_mem = 0
        for pid, mem in info.items():
            user_mem += mem
            if user_mem > 81074:
                os.system(f'kill {pid}')
                t = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())
                print(f'==> {t}  |  Killed {user}: {pid}')

    time.sleep(10)

```



## 服务器被黑应急响应



1. 首先查看进程是哪个用户造成的，看看该任务是否有 crontab 定时任务，不要急着把程序 kill 掉，有 crontab 的话把 crontab 里面的东西删掉，把脚本文件也删掉
2. 用 last 命令查看这个用户从哪个 ip 登录上服务器的，指不定是学校的内鬼或者其他肉鸡
3. 改密码，把弱密码给改了，通常爆破弱密码登录的可能性十分大，很多人用的都是默认和 id 一样的密码
4. 如果是新建的用户则删除用户，并且禁止该用户通过 ssh 登录服务器，如果知道 ip 的话顺便把 ip 也给禁了
5. 毕了业的师兄师姐的账号 userdel ，删除用户之后东西都还在，只是不能通过 id 登录了



```bash
sudo vim /etc/ssh/sshd_config
(在里面添加一行 DenyUsers user1 user2)

sudo systemctl restart sshd.service
(重启 sshd 使改动生效)

sudo journalctl -xe
(查看重启日志)
```



查看最近用户登录情况

| 命令      | 日志文件         | 功能                        |
| --------- | ---------------- | --------------------------- |
| `last`    | /var/log/wtmp    | 所有成功登录/登出的历史记录 |
| `lastb`   | /var/log/btmp    | 登录失败尝试                |
| `lastlog` | /var/log/lastlog | 最近登录记录                |



如果想清除记录的话直接 `echo > /var/log/wtmp` 写入空文件就行



## This account is currently unavailable



症状：用某个用户的 id 和密码进行 ssh 登陆，然后就报了这个错误，开始以为是这个用户被删除了，然后看了一下并没有，后来查了一下，发现这用户被禁止登陆了，`cat /etc/passwd | grep user`，发现他的 shell 是 `/sbin/nologin` ，将其改成 `/bin/bash` 就可以了，所以学到了一招，以后想禁止哪个用户登陆直接修改他的 shell 为 `/sbin/nologin` 就行了。



## 233 网段多台服务器没关机但是连不上



这个很难发现，233网段好多台机器都连不上了，但是有些又是好的，具体情况是有些机器网口没有亮灯，有些闪烁橙色灯，重启了也不行，所以怀疑就是网络问题，经过万总的经验，判断出来网口连着的交换机灯没有亮，判断是这台交换机有问题，然后换了一台好的交换机还是不行，这时想起来我们连着的交换机可能是这条线路汇总的总交换机，总的都没用，插其他的肯定也不行，最后就将那台交换机给开机了，然后这条线上的所有机器都好了。



## 安全相关



1. 除非有特殊需要，否则禁止普通用户的 docker 权限！
2. 设置密码时不要太简单，应使用大小写字母加数字加特殊字符的组合
3. 校外访问如需用到内网穿透服务（如 frp，ngork 等），配置连接时应使用加密协议



某运维工程师的记录 https://zhuanlan.zhihu.com/p/441837141



